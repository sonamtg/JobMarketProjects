---
title: "tidy_script"
author: "Al Ashir Intisar"
date: "5/2/2023"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# #This section separates the scripts based on character and episodes for text analysis on character level.
library(readr)
untidy_scripts <- read_csv("~/Mscs 264 S23/Project/Final Project Sonam-Inti/untidy_scripts.csv")

#character_lines <- read_csv("Mscs 264 S23/Project/Final Project Sonam-Inti/character_lines.csv")

library(tidyverse)

pattern <- "s\\d{2}e\\d{2}"


Episode <- c("Title")
Character <- c("Character")
Lines <- c("Lines")

character_lines <- tibble(Episode, Character, Lines)




for (i in 2:148) {

  Episode <- str_extract(untidy_scripts$Title[i], "s\\d{2}e\\d{2}")|>
    unlist()

  Character <- str_extract_all(untidy_scripts$Script[i], "\\w[A-Z]+:")|>
  unlist()

  Lines <- unlist(strsplit(untidy_scripts$Script[i], "\\w[A-Z]+:"))[-1]

  new <-   tibble(Episode, Character, Lines)

  character_lines <- rbind(character_lines, new)


}



write_csv(character_lines, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/character_lines.csv")
```


```{r}
#create word counts for each seasons
library(tidytext)

untidy_scripts <- read_csv("~/Mscs 264 S23/Project/Final Project #Sonam-Inti/untidy_scripts.csv")

word_count_episode <- untidy_scripts|>
  unnest_tokens(word, Script)|>
  group_by(Title)|>
  summarise(word_count = n())|>
  mutate(season = parse_number(str_sub(Title, 13, 14)))|>
  mutate(episode = parse_number(str_sub(Title, 16, 17)))|>
  select(-Title)

word_ct_episode <- word_count_episode|>
  drop_na()

write_csv(word_ct_episode, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/word_ct_episode.csv")
```


```{r}
library(dplyr)
# #adding the release date to the dataset
#
# #reading in the word count dataset
word_ct_episode <- read_csv("~/Mscs 264 S23/Project/Final Project Sonam-Inti/word_ct_episode.csv")

#getting rid of season 21 and creating unique id for each episode
word_ct_episode <- word_ct_episode|>
  filter(season <21)|>
  mutate(id= str_c("S", season, "e", episode))|>
  select(word_count, id)

#reading in the imdb dataset
season1_20_ep1_7 <- read_csv("~/Mscs 264 S23/Project/Final Project Sonam-Inti/season1-20-ep1-7.csv")

#creating a unique id variable for each episode for the tidy dataset
season1_20_ep1_7 <- season1_20_ep1_7|>
  mutate(id = str_c(season_num, "e",episode_num))

tidy_family_guy <- full_join(word_ct_episode, season1_20_ep1_7, by = "id")|>
  select(id, everything())


tidy_family_guy|>
  # group_by(episode)|>
  # summarise(sum_words = sum(word_count))|>
  ggplot(aes(x= air_date, y = word_count))+
  geom_point()+
  geom_smooth()

write_csv(tidy_family_guy, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/tidy_family_guy.csv")

```


```{r}
#the character lines is a failure cause not all the scripts has the character patter.
#only adding character lines to the seasons that are available

character_lines <- read_csv("~/Mscs 264 S23/Project/Final Project Sonam-Inti/character_lines.csv")


character_lines <-  character_lines|>
  mutate(season = parse_number(str_sub(Episode, 2,3)), episode = parse_number(str_sub(Episode, 5, 6)))|>
  drop_na()|>
  mutate(id = str_c("S", season, "e", episode))|>
  select(id, Character, Lines)

write_csv(character_lines, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/character_lines.csv")

tidy_family_guy <- read_csv("~/Mscs 264 S23/Project/Final Project Sonam-Inti/tidy_family_guy.csv")
character_lines <- read_csv("Mscs 264 S23/Project/Final Project Sonam-Inti/character_lines.csv")

char_ep1 <- character_lines|>
  group_by(id)|>
  summarise(num_char = n())|>
  arrange(desc(num_char))|>
  filter(str_detect(id, "^S1e"))



tidy_family_guy <- full_join(tidy_family_guy, char_ep1, by = "id")
write_csv(tidy_family_guy, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/tidy_family_guy.csv")


#character Lines sentiment for season 1
#for sonam

#number of words per character in season 1

char_words_s1 <- character_lines|>
  unnest_tokens(word, Lines)|>
  group_by(id, Character)|>
  summarise(char_words = n())|>
  arrange(desc(char_words))|>
  filter(str_detect(id, "^S1e"))


write_csv(char_words_s1, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/char_words_s1.csv")


tidy_family_guy|>
  ggplot(aes(x = air_date, y = num_ratings))+
  geom_point()

```


```{r}
#upload a list of characters for the family guy and calculate the frequency of it. 
#only do character sentiment analysis in the first season. cause only first season has the #character patter. 


```

**making id by creating a variable season_num and only keeping the columns Script and id for joining**

```{r}
untidy_scripts <- untidy_scripts %>%
  mutate(season_num = str_sub(Title, 13, 14)) %>%
  mutate(season_num = as.numeric(season_num)) %>%
  mutate(season_num = as.character(season_num)) %>%
  mutate(id = str_c("S", season_num, "e", str_sub(Title, 17, 17), sep = "")) %>%
  select(Script, id)
  
```

**Joining the tidy_family_guy data set with the script**

```{r}
untidy_full_family_guy <- tidy_family_guy %>%
  full_join(untidy_scripts, by = "id") %>%
  drop_na(season_num)
```


```{r}
untidy_full_family_guy <- untidy_full_family_guy %>%
  unnest_tokens(word, Script) %>%
  anti_join(get_stopwords(source = "smart"))
```

**Calculating the tf-idf of all the seasons**

```{r}
family_guy_tfidf <- all_words %>%
  count(season_num, episode_num, word) %>%
  bind_tf_idf(word, season_num, n)

# since values with higher tfidf provide more information as they are rare words and provide more information

family_guy_tfidf %>%
  arrange(-tf_idf)
  

```


```{r}
family_guy_tfidf %>%
  group_by(season_num, episode_num) %>%
  filter(season_num == "S2", episode_num == 4) %>%
  arrange(desc(tf_idf)) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup %>%
  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = season_num)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    facet_wrap(~season_num, scales = "free")
```



```{r}
write_csv(untidy_full_family_guy, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/all_words.csv")
write_csv(family_guy_tfidf, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/family_guy_tfidf.csv")


```


```{r}
text <- "PETER: No, thanks. See, that's the worst we got is Jemima's Witnesses. LOIS: [Singing.] It seems today that all you see is violence in movies and sex on TV "

#Split text by character names

split_text <- unlist(strsplit(text, "(?<=\\w+:) "))

speakers <- regmatches(content, gregexpr("\\w[A-Z]+:", content))|>

# Print each character's lines
for (i in 1:length(split_text)) {
  print(split_text[i])
}
```

**Writing all the lex csv files into the shiny folder**

```{r}
lex_afinn <- get_sentiments(lexicon = "afinn")
write_csv(lex_afinn, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/shiny/lex_afinn.csv")

lex_nrc <- get_sentiments(lexicon = "nrc")
write_csv(lex_nrc, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/shiny/lex_nrc.csv")

lex_bing <- get_sentiments(lexicon = "bing")
write_csv(lex_bing, "~/Mscs 264 S23/Project/Final Project Sonam-Inti/shiny/lex_bing.csv")

```


```{r}
# x <- "one,two,three"
# strsplit(x, "(?<=,)", perl=TRUE)

```

